{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple ASI Core **:- Hereâ€™s the code for a Reinforcement Learning (RL) agent learning to maximize a reward through interactions with an environment. We'll use a Q-learning agent with a simple environment to simulate self-improvement and feedback loops."
      ],
      "metadata": {
        "id": "FstH3GdQOBbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srs0SbtEMvWa",
        "outputId": "5b9a2e13-8f0a-493d-c65d-2f42f50fc424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 - Total Reward: 10\n",
            "Current Q-table:\n",
            " [[-0.1         0.1       ]\n",
            " [ 0.95617925  0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 100 - Total Reward: 54\n",
            "Current Q-table:\n",
            " [[2.29151217 9.99925825]\n",
            " [9.99999973 0.        ]\n",
            " [0.         0.        ]]\n",
            "Episode 200 - Total Reward: 8\n",
            "Current Q-table:\n",
            " [[ 4.62916305  9.99999998]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 300 - Total Reward: 26\n",
            "Current Q-table:\n",
            " [[ 6.38773914 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 400 - Total Reward: 8\n",
            "Current Q-table:\n",
            " [[ 7.4378394 10.       ]\n",
            " [10.         0.       ]\n",
            " [ 0.         0.       ]]\n",
            "Episode 500 - Total Reward: 19\n",
            "Current Q-table:\n",
            " [[ 7.73112033 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 600 - Total Reward: 2\n",
            "Current Q-table:\n",
            " [[ 7.87139569 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 700 - Total Reward: 2\n",
            "Current Q-table:\n",
            " [[ 7.91562271 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 800 - Total Reward: 2\n",
            "Current Q-table:\n",
            " [[ 7.98974169 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n",
            "Episode 900 - Total Reward: 11\n",
            "Current Q-table:\n",
            " [[ 7.99602572 10.        ]\n",
            " [10.          0.        ]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment Setup\n",
        "class SimpleEnvironment:\n",
        "    def __init__(self):\n",
        "        self.states = [0, 1, 2]  # Three possible states\n",
        "        self.actions = [0, 1]  # Two possible actions (0 = bad action, 1 = good action)\n",
        "        self.reward_matrix = {\n",
        "            (0, 0): -1, (0, 1): 1,  # State 0\n",
        "            (1, 0): 1, (1, 1): 0,   # State 1\n",
        "            (2, 0): 0, (2, 1): 10,  # State 2 (goal state)\n",
        "        }\n",
        "        self.current_state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = self.reward_matrix.get((self.current_state, action), -1)\n",
        "        # Transition to a new state based on action\n",
        "        if action == 1:  # Good action: move to next state\n",
        "            next_state = min(self.current_state + 1, len(self.states) - 1)\n",
        "        else:  # Bad action: stay in the current state\n",
        "            next_state = self.current_state\n",
        "        self.current_state = next_state\n",
        "        return next_state, reward\n",
        "\n",
        "# Q-Learning Agent Setup\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = np.zeros((len(env.states), len(env.actions)))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:  # Exploration\n",
        "            return random.choice([0, 1])\n",
        "        else:  # Exploitation\n",
        "            return np.argmax(self.q_table[state])  # Select best action based on Q-table\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        old_q_value = self.q_table[state, action]\n",
        "        future_q_value = np.max(self.q_table[next_state])\n",
        "        # Update Q-value using the Q-learning formula\n",
        "        self.q_table[state, action] = old_q_value + self.alpha * (reward + self.gamma * future_q_value - old_q_value)\n",
        "\n",
        "# Simulation of Learning Process with Feedback Loop\n",
        "def run_simulation(episodes=1000):\n",
        "    env = SimpleEnvironment()\n",
        "    agent = QLearningAgent(env)\n",
        "\n",
        "    # Simulate the learning process\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Check if the agent has reached the goal state\n",
        "            if state == 2:  # Goal state\n",
        "                done = True\n",
        "\n",
        "        # Feedback Loop: Simulate Human Feedback\n",
        "        if total_reward < 5:  # Poor performance feedback (human teaching scenario)\n",
        "            agent.epsilon = 0.2  # More exploration to improve learning\n",
        "        else:  # Positive feedback, continue with current exploration/exploitation balance\n",
        "            agent.epsilon = 0.1\n",
        "\n",
        "        # Print status every 100 episodes\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode} - Total Reward: {total_reward}\")\n",
        "            print(\"Current Q-table:\\n\", agent.q_table)\n",
        "\n",
        "# Run the simulation\n",
        "run_simulation(episodes=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **My Idea with code generated by ChatGPT, executed by Bhadale IT**"
      ],
      "metadata": {
        "id": "7-pkTLekOYS6"
      }
    }
  ]
}